# Uses the learning rate schedule defined in https://arxiv.org/abs/1706.03762.

params:
  optimizer: AdamOptimizer
  learning_rate: 1.0 # The scale constant.
  decay_type: noam_decay
  decay_rate: 512 # Model dimension.
  decay_steps: 16000 # Warmup steps.
  start_decay_steps: 0
